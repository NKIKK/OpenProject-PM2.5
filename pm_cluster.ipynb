{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext,SparkConf\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "# SparkContext(\"local\", \"App Name\").memory.offHeap.enabled\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master('local')\\\n",
    "        .appName('SparkSQL')\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "config = SparkConf().setAll([('spark.executor.memory', '8g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3'), ('spark.driver.memory','8g')])\n",
    "sc.stop()\n",
    "sc = SparkContext(conf=config)\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "\n",
    "# spark.memory.offHeap.enabled\n",
    "# spark.memory.offHeap.size\n",
    "\n",
    "# spark.config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
    "#         .config(\"spark.memory.offHeap.size\",\"10g\")\n",
    "spark = SparkSession.builder\\\n",
    "        .master('local')\\\n",
    "        .appName('SparkSQL')\\\n",
    "        .getOrCreate()\n",
    "df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv('pm_clean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name_th: string (nullable = true)\n",
      " |-- name_en: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- time_aq: string (nullable = true)\n",
      " |-- temp: string (nullable = true)\n",
      " |-- humid: string (nullable = true)\n",
      " |-- pm25_corrected: string (nullable = true)\n",
      " |-- pm10_corrected: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import *\n",
    "cols = ['lat', 'long','temp','humid','pm25_corrected','pm10_corrected']\n",
    "for c in cols:\n",
    "    df = df.withColumn(c, col(c).cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name_th: string (nullable = true)\n",
      " |-- name_en: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- time_aq: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- humid: double (nullable = true)\n",
      " |-- pm25_corrected: double (nullable = true)\n",
      " |-- pm10_corrected: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+-------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+\n",
      "|      device_id| id|            name_th|             name_en|      lat|      long|_c0|             time_aq|temp|humid|pm25_corrected|pm10_corrected|\n",
      "+---------------+---+-------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084|  2|2020-07-20 18:17:...|33.1| 67.9|          11.2|          23.1|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084|  6|2020-07-20 18:22:...|33.1| 69.3|          10.4|          21.6|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 10|2020-07-20 18:27:...|32.9| 70.5|           9.7|          20.9|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 14|2020-07-20 18:32:...|32.9| 71.2|           8.9|          19.4|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 18|2020-07-20 18:37:...|32.8| 71.6|           8.9|          19.4|\n",
      "+---------------+---+-------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "# df3 = df.withColumn(\n",
    "#             'new_date',\n",
    "#                 F.to_date(\n",
    "#                     F.unix_timestamp('time_aq', 'yyyy-MM-dd HH:mm:ss:SSS').cast('timestamp')))\n",
    "# df3=df.withColumn(\"time_aq\",to_timestamp(\"input_timestamp\")).show(truncate=False)\n",
    "df3=df.withColumn(\"new_time\",to_timestamp(\"time_aq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+--------------------+--------------------+---------+----------+-------+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "|      device_id| id|             name_th|             name_en|      lat|      long|    _c0|             time_aq|temp|humid|pm25_corrected|pm10_corrected|            new_time|\n",
      "+---------------+---+--------------------+--------------------+---------+----------+-------+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "|868333031252642| 82|ศูนย์ศึกษาการพัฒน...|Huai Hong Khrai R...| 18.87813|  99.21817|3593250|2021-06-29 08:48:...|31.6| 63.5|          10.4|          23.1|2021-06-29 08:48:...|\n",
      "|868333031266329| 67|(4) อาคาร 11 โครง...|(4) Building 11 B...| 13.60095| 100.74824|3908067|2021-07-09 05:54:...|32.2| 69.0|          14.1|          29.1|2021-07-09 05:54:...|\n",
      "|868333031254515| 77|ซอยเพชรเกษม 43 บางแค|Soi Phetkasem 43,...|13.708905| 100.41382|5714048|2021-09-01 03:24:...|29.2| 94.0|          42.4|          84.2|2021-09-01 03:24:...|\n",
      "|868333031256858|160|        เขื่อนบางลาง|       Bang Lang Dam|  6.15516|101.272477|1681275|2021-04-03 00:52:...|32.4| 54.0|           8.9|          20.1|2021-04-03 00:52:...|\n",
      "|868333031268465|212|โรงเรียนศรีสะเกษว...|Sisaketwittayalai...|15.111844|104.324492|4360518|2021-07-22 20:18:...|33.8| 56.4|          15.6|          32.8|2021-07-22 20:18:...|\n",
      "+---------------+---+--------------------+--------------------+---------+----------+-------+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name_th: string (nullable = true)\n",
      " |-- name_en: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- time_aq: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- humid: double (nullable = true)\n",
      " |-- pm25_corrected: double (nullable = true)\n",
      " |-- pm10_corrected: double (nullable = true)\n",
      " |-- new_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6700545b2824>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'new_date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2020-07-20 18:35:00'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munix_timestamp\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'yyyy-MM-dd HH:mm:ss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\functions.py\u001b[0m in \u001b[0;36mto_date\u001b[1;34m(col, format)\u001b[0m\n\u001b[0;32m   2001\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2002\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2003\u001b[1;33m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2004\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1313\u001b[1;33m         \u001b[0margs_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1277\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m             \u001b[0mnew_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m_get_args\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m   1262\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mconverter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcan_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1264\u001b[1;33m                         \u001b[0mtemp_arg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1265\u001b[0m                         \u001b[0mtemp_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m                         \u001b[0mnew_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_collections.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, object, gateway_client)\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[0mArrayList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJavaClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"java.util.ArrayList\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[0mjava_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArrayList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m             \u001b[0mjava_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjava_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\column.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Column is not iterable\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[1;31m# string methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "# df3.filter(df3['new_date'] >= F.to_date('2020-07-20 18:35:00',F.unix_timestamp( 'yyyy-MM-dd HH:mm:ss').cast('timestamp'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2020-07-20|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select to_date('2020-07-20 18:35:00','yyyy-MM-dd HH:mm:ss') date\") \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df4=spark.createDataFrame([[\"2020-07-20 18:35:00.000\"],[\"2020-07-20 18:40:00.000\"]],[\"input\"])\n",
    "df5 = df4.withColumn(\n",
    "            'new_date',\n",
    "                F.to_date(\n",
    "                    F.unix_timestamp('input', 'yyyy-MM-dd HH:mm:ss:SSS').cast('timestamp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|               input|                new|\n",
      "+--------------------+-------------------+\n",
      "|2020-07-20 18:35:...|2020-07-20 18:35:00|\n",
      "|2020-07-20 18:40:...|2020-07-20 18:40:00|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5=df4.withColumn(\"new\",to_timestamp(\"input\"))\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               input|\n",
      "+--------------------+\n",
      "|2020-07-20 18:35:...|\n",
      "|2020-07-20 18:40:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|               input|                new|\n",
      "+--------------------+-------------------+\n",
      "|2020-07-20 18:35:...|2020-07-20 18:35:00|\n",
      "|2020-07-20 18:40:...|2020-07-20 18:40:00|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 7, 20, 18, 35)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.collect()[0]['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+--------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "|      device_id| id|             name_th|             name_en|      lat|      long|_c0|             time_aq|temp|humid|pm25_corrected|pm10_corrected|            new_time|\n",
      "+---------------+---+--------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "|868333030868281| 24| *****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 18|2020-07-20 18:37:...|32.8| 71.6|           8.9|          19.4|2020-07-20 18:37:...|\n",
      "|868333030873646| 23|*****กรมประชาสัมพ...|Government Public...|13.783098|100.540368| 16|2020-07-20 18:37:...|31.4| 79.1|           9.7|          20.9|2020-07-20 18:37:...|\n",
      "|   30AEA49CC204| 17|ตึก 3 คณะวิศวกรรม...|Building 3, Facul...|13.736983|100.533476| 17|2020-07-20 18:37:...|35.7| 62.0|           8.5|           9.2|2020-07-20 18:37:...|\n",
      "|868333030879387| 22|*****สถานีทดสอบที่ 1|   Testing station 1|13.824576|100.579485| 19|2020-07-20 18:38:...|29.3| 83.4|          12.6|          26.8|2020-07-20 18:38:...|\n",
      "+---------------+---+--------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.filter((df3['new_time'] >= df5.collect()[0]['new']) & (df3['new_time'] < df5.collect()[1]['new'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df4.select(col(\"input\"), \n",
    "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\") \n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "dff = spark.createDataFrame(data=data2,schema=schema)\n",
    "dff.printSchema()\n",
    "dff.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+-------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+--------+\n",
      "|      device_id| id|            name_th|             name_en|      lat|      long|_c0|             time_aq|temp|humid|pm25_corrected|pm10_corrected|new_date|\n",
      "+---------------+---+-------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+--------+\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084|  2|2020-07-20 18:17:...|33.1| 67.9|          11.2|          23.1|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084|  6|2020-07-20 18:22:...|33.1| 69.3|          10.4|          21.6|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 10|2020-07-20 18:27:...|32.9| 70.5|           9.7|          20.9|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 14|2020-07-20 18:32:...|32.9| 71.2|           8.9|          19.4|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 18|2020-07-20 18:37:...|32.8| 71.6|           8.9|          19.4|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 21|2020-07-20 18:42:...|32.7| 72.3|           8.9|          17.9|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 25|2020-07-20 18:47:...|32.7| 72.9|           9.7|          20.1|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 29|2020-07-20 18:52:...|32.6| 73.2|           8.9|          19.4|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 33|2020-07-20 18:57:...|32.5| 73.0|           8.9|          18.6|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 37|2020-07-20 19:02:...|32.5| 73.2|           8.9|          18.6|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 40|2020-07-20 19:07:...|32.5| 73.3|           8.2|          17.9|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 44|2020-07-20 19:12:...|32.4| 73.2|           8.2|          17.9|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 48|2020-07-20 19:17:...|32.4| 72.9|           8.2|          17.9|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 52|2020-07-20 19:22:...|32.3| 73.2|           8.2|          17.1|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 56|2020-07-20 19:27:...|32.3| 73.5|           8.2|          17.9|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 60|2020-07-20 19:32:...|32.3| 73.2|           8.2|          17.1|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 64|2020-07-20 19:37:...|32.4| 73.2|           8.9|          17.9|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 68|2020-07-20 19:42:...|32.4| 73.1|           8.9|          18.6|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 72|2020-07-20 19:47:...|32.4| 72.9|           8.9|          18.6|    null|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 76|2020-07-20 19:52:...|32.4| 72.8|           8.9|          18.6|    null|\n",
      "+---------------+---+-------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "data_customer.columns\n",
    "assemble=VectorAssembler(inputCols=[\n",
    " 'BALANCE',\n",
    " 'BALANCE_FREQUENCY',\n",
    " 'PURCHASES',\n",
    " 'ONEOFF_PURCHASES',\n",
    " 'INSTALLMENTS_PURCHASES',\n",
    " 'CASH_ADVANCE',\n",
    " 'PURCHASES_FREQUENCY',\n",
    " 'ONEOFF_PURCHASES_FREQUENCY',\n",
    " 'PURCHASES_INSTALLMENTS_FREQUENCY',\n",
    " 'CASH_ADVANCE_FREQUENCY',\n",
    " 'CASH_ADVANCE_TRX',\n",
    " 'PURCHASES_TRX',\n",
    " 'CREDIT_LIMIT',\n",
    " 'PAYMENTS',\n",
    " 'MINIMUM_PAYMENTS',\n",
    " 'PRC_FULL_PAYMENT',\n",
    " 'TENURE'], outputCol='features')\n",
    "assembled_data=assemble.transform(data_customer)\n",
    "assembled_data.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scale=StandardScaler(inputCol='features',outputCol='standardized')\n",
    "data_scale=scale.fit(assembled_data)\n",
    "data_scale_output=data_scale.transform(assembled_data)\n",
    "data_scale_output.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "silhouette_score=[]\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='standardized', \\\n",
    "                                metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "for i in range(2,10):\n",
    "\n",
    "    KMeans_algo=KMeans(featuresCol='standardized', k=i)\n",
    "\n",
    "    KMeans_fit=KMeans_algo.fit(data_scale_output)\n",
    "\n",
    "    output=KMeans_fit.transform(data_scale_output)\n",
    "\n",
    "\n",
    "\n",
    "    score=evaluator.evaluate(output)\n",
    "\n",
    "    silhouette_score.append(score)\n",
    "\n",
    "    print(\"Silhouette Score:\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the silhouette scores in a plot\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,10),silhouette_score)\n",
    "ax.set_xlabel(‘k’)\n",
    "ax.set_ylabel(‘cost’)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef98beed530724fd48f935abab518c0f3e16171ba4883f2358360793e5698786"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
