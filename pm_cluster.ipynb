{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext,SparkConf\n",
    "# SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "# SparkContext(\"local\", \"App Name\").memory.offHeap.enabled\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master('local')\\\n",
    "        .appName('SparkSQL')\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# config = SparkConf().setAll([('spark.executor.memory', '8g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3'), ('spark.driver.memory','8g')])\n",
    "# sc.stop()\n",
    "# sc = SparkContext(conf=config)\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "\n",
    "# spark.memory.offHeap.enabled\n",
    "# spark.memory.offHeap.size\n",
    "\n",
    "# spark.config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
    "#         .config(\"spark.memory.offHeap.size\",\"10g\")\n",
    "# spark = SparkSession.builder\\\n",
    "#         .master('local')\\\n",
    "#         .appName('SparkSQL')\\\n",
    "#         .getOrCreate()\n",
    "df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv('pm_clean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name_th: string (nullable = true)\n",
      " |-- name_en: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- time_aq: string (nullable = true)\n",
      " |-- temp: string (nullable = true)\n",
      " |-- humid: string (nullable = true)\n",
      " |-- pm25_corrected: string (nullable = true)\n",
      " |-- pm10_corrected: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import *\n",
    "cols = ['lat', 'long','temp','humid','pm25_corrected','pm10_corrected']\n",
    "for c in cols:\n",
    "    df = df.withColumn(c, col(c).cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name_th: string (nullable = true)\n",
      " |-- name_en: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- time_aq: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- humid: double (nullable = true)\n",
      " |-- pm25_corrected: double (nullable = true)\n",
      " |-- pm10_corrected: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+-------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+\n",
      "|      device_id| id|            name_th|             name_en|      lat|      long|_c0|             time_aq|temp|humid|pm25_corrected|pm10_corrected|\n",
      "+---------------+---+-------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084|  2|2020-07-20 18:17:...|33.1| 67.9|          11.2|          23.1|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084|  6|2020-07-20 18:22:...|33.1| 69.3|          10.4|          21.6|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 10|2020-07-20 18:27:...|32.9| 70.5|           9.7|          20.9|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 14|2020-07-20 18:32:...|32.9| 71.2|           8.9|          19.4|\n",
      "|868333030868281| 24|*****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 18|2020-07-20 18:37:...|32.8| 71.6|           8.9|          19.4|\n",
      "+---------------+---+-------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "# df3 = df.withColumn(\n",
    "#             'new_date',\n",
    "#                 F.to_date(\n",
    "#                     F.unix_timestamp('time_aq', 'yyyy-MM-dd HH:mm:ss:SSS').cast('timestamp')))\n",
    "# df3=df.withColumn(\"time_aq\",to_timestamp(\"input_timestamp\")).show(truncate=False)\n",
    "df3=df.withColumn(\"new_time\",to_timestamp(\"time_aq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+--------------------+--------------------+---------+----------+-------+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "|      device_id| id|             name_th|             name_en|      lat|      long|    _c0|             time_aq|temp|humid|pm25_corrected|pm10_corrected|            new_time|\n",
      "+---------------+---+--------------------+--------------------+---------+----------+-------+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "|868333031252642| 82|ศูนย์ศึกษาการพัฒน...|Huai Hong Khrai R...| 18.87813|  99.21817|3593250|2021-06-29 08:48:...|31.6| 63.5|          10.4|          23.1|2021-06-29 08:48:...|\n",
      "|868333031266329| 67|(4) อาคาร 11 โครง...|(4) Building 11 B...| 13.60095| 100.74824|3908067|2021-07-09 05:54:...|32.2| 69.0|          14.1|          29.1|2021-07-09 05:54:...|\n",
      "|868333031254515| 77|ซอยเพชรเกษม 43 บางแค|Soi Phetkasem 43,...|13.708905| 100.41382|5714048|2021-09-01 03:24:...|29.2| 94.0|          42.4|          84.2|2021-09-01 03:24:...|\n",
      "|868333031256858|160|        เขื่อนบางลาง|       Bang Lang Dam|  6.15516|101.272477|1681275|2021-04-03 00:52:...|32.4| 54.0|           8.9|          20.1|2021-04-03 00:52:...|\n",
      "|868333031268465|212|โรงเรียนศรีสะเกษว...|Sisaketwittayalai...|15.111844|104.324492|4360518|2021-07-22 20:18:...|33.8| 56.4|          15.6|          32.8|2021-07-22 20:18:...|\n",
      "+---------------+---+--------------------+--------------------+---------+----------+-------+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name_th: string (nullable = true)\n",
      " |-- name_en: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- time_aq: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- humid: double (nullable = true)\n",
      " |-- pm25_corrected: double (nullable = true)\n",
      " |-- pm10_corrected: double (nullable = true)\n",
      " |-- new_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.filter(df3['new_date'] >= F.to_date('2020-07-20 18:35:00',F.unix_timestamp( 'yyyy-MM-dd HH:mm:ss').cast('timestamp'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2020-07-20|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select to_date('2020-07-20 18:35:00','yyyy-MM-dd HH:mm:ss') date\") \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df4=spark.createDataFrame([[\"2020-07-20 18:35:00.000\"],[\"2020-07-20 18:40:00.000\"]],[\"input\"])\n",
    "df5 = df4.withColumn(\n",
    "            'new_date',\n",
    "                F.to_date(\n",
    "                    F.unix_timestamp('input', 'yyyy-MM-dd HH:mm:ss:SSS').cast('timestamp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|               input|                new|\n",
      "+--------------------+-------------------+\n",
      "|2020-07-20 18:35:...|2020-07-20 18:35:00|\n",
      "|2020-07-20 18:40:...|2020-07-20 18:40:00|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5=df4.withColumn(\"new\",to_timestamp(\"input\"))\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               input|\n",
      "+--------------------+\n",
      "|2020-07-20 18:35:...|\n",
      "|2020-07-20 18:40:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|               input|                new|\n",
      "+--------------------+-------------------+\n",
      "|2020-07-20 18:35:...|2020-07-20 18:35:00|\n",
      "|2020-07-20 18:40:...|2020-07-20 18:40:00|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 7, 20, 18, 35)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.collect()[0]['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+--------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "|      device_id| id|             name_th|             name_en|      lat|      long|_c0|             time_aq|temp|humid|pm25_corrected|pm10_corrected|            new_time|\n",
      "+---------------+---+--------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "|868333030868281| 24| *****กรมทรัพยากรน้ำ|Department of Wat...|13.785348|100.539084| 18|2020-07-20 18:37:...|32.8| 71.6|           8.9|          19.4|2020-07-20 18:37:...|\n",
      "|868333030873646| 23|*****กรมประชาสัมพ...|Government Public...|13.783098|100.540368| 16|2020-07-20 18:37:...|31.4| 79.1|           9.7|          20.9|2020-07-20 18:37:...|\n",
      "|   30AEA49CC204| 17|ตึก 3 คณะวิศวกรรม...|Building 3, Facul...|13.736983|100.533476| 17|2020-07-20 18:37:...|35.7| 62.0|           8.5|           9.2|2020-07-20 18:37:...|\n",
      "|868333030879387| 22|*****สถานีทดสอบที่ 1|   Testing station 1|13.824576|100.579485| 19|2020-07-20 18:38:...|29.3| 83.4|          12.6|          26.8|2020-07-20 18:38:...|\n",
      "+---------------+---+--------------------+--------------------+---------+----------+---+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.filter((df3['new_time'] >= df5.collect()[0]['new']) & (df3['new_time'] < df5.collect()[1]['new'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df4.select(col(\"input\"), \n",
    "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\") \n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "dff = spark.createDataFrame(data=data2,schema=schema)\n",
    "dff.printSchema()\n",
    "dff.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+--------------------+--------------------+---------+----------+--------+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "|      device_id| id|             name_th|             name_en|      lat|      long|     _c0|             time_aq|temp|humid|pm25_corrected|pm10_corrected|            new_time|\n",
      "+---------------+---+--------------------+--------------------+---------+----------+--------+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "|868333031252642| 82|ศูนย์ศึกษาการพัฒน...|Huai Hong Khrai R...| 18.87813|  99.21817| 3593250|2021-06-29 08:48:...|31.6| 63.5|          10.4|          23.1|2021-06-29 08:48:...|\n",
      "|868333031266329| 67|(4) อาคาร 11 โครง...|(4) Building 11 B...| 13.60095| 100.74824| 3908067|2021-07-09 05:54:...|32.2| 69.0|          14.1|          29.1|2021-07-09 05:54:...|\n",
      "|868333031254515| 77|ซอยเพชรเกษม 43 บางแค|Soi Phetkasem 43,...|13.708905| 100.41382| 5714048|2021-09-01 03:24:...|29.2| 94.0|          42.4|          84.2|2021-09-01 03:24:...|\n",
      "|868333031256858|160|        เขื่อนบางลาง|       Bang Lang Dam|  6.15516|101.272477| 1681275|2021-04-03 00:52:...|32.4| 54.0|           8.9|          20.1|2021-04-03 00:52:...|\n",
      "|868333031268465|212|โรงเรียนศรีสะเกษว...|Sisaketwittayalai...|15.111844|104.324492| 4360518|2021-07-22 20:18:...|33.8| 56.4|          15.6|          32.8|2021-07-22 20:18:...|\n",
      "|868333031252642| 82|ศูนย์ศึกษาการพัฒน...|Huai Hong Khrai R...| 18.87813|  99.21817| 1480789|2021-03-17 14:27:...|42.8| 30.0|          48.3|          82.0|2021-03-17 14:27:...|\n",
      "|868333031256858|160|        เขื่อนบางลาง|       Bang Lang Dam|  6.15516|101.272477| 3522943|2021-06-26 21:14:...|30.1| 82.0|           8.9|          20.1|2021-06-26 21:14:...|\n",
      "|868333031266485|126|โรงเรียนเทศบาลคำใ...|Khamyai Punnamjai...|16.672226|102.764458| 6883074|2021-10-05 23:23:...|30.7| 71.2|          11.2|          23.8|2021-10-05 23:23:...|\n",
      "|868333031252642| 82|ศูนย์ศึกษาการพัฒน...|Huai Hong Khrai R...| 18.87813|  99.21817|  503047|2020-12-29 11:14:...|29.6| 42.9|          20.8|          39.5|2020-12-29 11:14:...|\n",
      "|868333031254812|251|   เทศบาลตำบลเวียงสา|   Wiang Sa Building|18.570911|100.751085|10798047|2022-01-08 04:54:...|22.3| 63.2|          40.1|          71.5|2022-01-08 04:54:...|\n",
      "|868333031259571|208|     โรงเรียนบ้านไผ่|      Banphai School|16.072254|102.733283|10632273|2022-01-05 05:34:...|24.1| 75.9|          25.3|          48.4|2022-01-05 05:34:...|\n",
      "|868333031259027|167|โรงเรียนอนุบาลวัด...|Anubanwatsakaeo S...|13.819968|102.066574| 6528507|2021-09-25 16:40:...|32.8| 67.6|          11.9|          26.1|2021-09-25 16:40:...|\n",
      "|868333031251800| 75|(12) อาคารสำนักงา...|(12) Bangchalong ...| 13.60125| 100.74993| 8842515|2021-11-28 09:49:...|31.4| 77.3|          24.5|          45.4|2021-11-28 09:49:...|\n",
      "|868333031258722|114|สถานีสูบน้ำหนองแค...|Nong Khae pumping...|14.331443|100.872881| 8403025|2021-11-16 21:36:...|31.0| 74.4|          21.6|          42.5|2021-11-16 21:36:...|\n",
      "|868333031264662|189| โรงไฟฟ้าบ้านขุนกลาง|Ban Khun Klang Po...|18.541825| 98.517977| 8374059|2021-11-16 03:10:...|23.7| 61.1|           8.9|          20.1|2021-11-16 03:10:...|\n",
      "|868333031268465|212|โรงเรียนศรีสะเกษว...|Sisaketwittayalai...|15.111844|104.324492| 3918447|2021-07-09 13:33:...|41.0| 41.7|           9.7|          21.6|2021-07-09 13:33:...|\n",
      "|868333031254796|108|คุณสายชล สันเขื่อ...|         Sirikit Dam|17.767062|100.566225| 2496382|2021-05-18 12:10:...|38.4| 49.3|          20.8|          39.5|2021-05-18 12:10:...|\n",
      "|868333031267426|142|สถานีตรวจวัดคุณภา...|Ban Klongwailek A...| 7.962033|  99.05563| 7885703|2021-11-03 02:45:...|27.1| 92.0|          16.4|          39.5|2021-11-03 02:45:...|\n",
      "|868333031259068|185|โรงเรียนศรีสงคราม...|Srisongkramwittay...|17.300882|101.790582| 6987465|2021-10-08 22:07:...|31.3| 71.8|          30.5|          55.9|2021-10-08 22:07:...|\n",
      "|868333031230143|322|ห้างหุ้นส่วนจำกัด...|Nopbhadung Servic...|16.551685|104.719146|11709268|2022-01-21 19:03:...|27.4| 55.3|          52.8|          93.1|2022-01-21 19:03:...|\n",
      "+---------------+---+--------------------+--------------------+---------+----------+--------+--------------------+----+-----+--------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['device_id',\n",
       " 'id',\n",
       " 'name_th',\n",
       " 'name_en',\n",
       " 'lat',\n",
       " 'long',\n",
       " '_c0',\n",
       " 'time_aq',\n",
       " 'temp',\n",
       " 'humid',\n",
       " 'pm25_corrected',\n",
       " 'pm10_corrected',\n",
       " 'new_time']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+--------------------+--------------------+--------+---------+-------+--------------------+----+-----+--------------+--------------+--------------------+--------------------+\n",
      "|      device_id| id|             name_th|             name_en|     lat|     long|    _c0|             time_aq|temp|humid|pm25_corrected|pm10_corrected|            new_time|            features|\n",
      "+---------------+---+--------------------+--------------------+--------+---------+-------+--------------------+----+-----+--------------+--------------+--------------------+--------------------+\n",
      "|868333031252642| 82|ศูนย์ศึกษาการพัฒน...|Huai Hong Khrai R...|18.87813| 99.21817|3593250|2021-06-29 08:48:...|31.6| 63.5|          10.4|          23.1|2021-06-29 08:48:...|[18.87813,99.2181...|\n",
      "|868333031266329| 67|(4) อาคาร 11 โครง...|(4) Building 11 B...|13.60095|100.74824|3908067|2021-07-09 05:54:...|32.2| 69.0|          14.1|          29.1|2021-07-09 05:54:...|[13.60095,100.748...|\n",
      "+---------------+---+--------------------+--------------------+--------+---------+-------+--------------------+----+-----+--------------+--------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# df3.columns\n",
    "assemble=VectorAssembler(inputCols=[\n",
    " \n",
    " 'lat',\n",
    " 'long',\n",
    " \n",
    " 'temp',\n",
    " 'humid',\n",
    " 'pm25_corrected',\n",
    " 'pm10_corrected',\n",
    " ], outputCol='features')\n",
    "assembled_data=assemble.transform(df3)\n",
    "assembled_data.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-----+--------------+--------------+\n",
      "|     lat|     long|temp|humid|pm25_corrected|pm10_corrected|\n",
      "+--------+---------+----+-----+--------------+--------------+\n",
      "|18.87813| 99.21817|31.6| 63.5|          10.4|          23.1|\n",
      "|13.60095|100.74824|32.2| 69.0|          14.1|          29.1|\n",
      "+--------+---------+----+-----+--------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3[[\n",
    " \n",
    " 'lat',\n",
    " 'long',\n",
    " \n",
    " 'temp',\n",
    " 'humid',\n",
    " 'pm25_corrected',\n",
    " 'pm10_corrected',\n",
    " ]]\n",
    "df4.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+--------------------+--------------------+--------+---------+-------+--------------------+----+-----+--------------+--------------+--------------------+--------------------+--------------------+\n",
      "|      device_id| id|             name_th|             name_en|     lat|     long|    _c0|             time_aq|temp|humid|pm25_corrected|pm10_corrected|            new_time|            features|        standardized|\n",
      "+---------------+---+--------------------+--------------------+--------+---------+-------+--------------------+----+-----+--------------+--------------+--------------------+--------------------+--------------------+\n",
      "|868333031252642| 82|ศูนย์ศึกษาการพัฒน...|Huai Hong Khrai R...|18.87813| 99.21817|3593250|2021-06-29 08:48:...|31.6| 63.5|          10.4|          23.1|2021-06-29 08:48:...|[18.87813,99.2181...|[7.09875768964063...|\n",
      "|868333031266329| 67|(4) อาคาร 11 โครง...|(4) Building 11 B...|13.60095|100.74824|3908067|2021-07-09 05:54:...|32.2| 69.0|          14.1|          29.1|2021-07-09 05:54:...|[13.60095,100.748...|[5.11437565049704...|\n",
      "+---------------+---+--------------------+--------------------+--------+---------+-------+--------------------+----+-----+--------------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scale=StandardScaler(inputCol='features',outputCol='standardized')\n",
    "data_scale=scale.fit(assembled_data)\n",
    "data_scale_output=data_scale.transform(assembled_data)\n",
    "data_scale_output.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.2476230855336728\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "silhouette_score=[]\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='standardized', \\\n",
    "                                metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "for i in range(2,10):\n",
    "\n",
    "    KMeans_algo=KMeans(featuresCol='standardized', k=i)\n",
    "\n",
    "    KMeans_fit=KMeans_algo.fit(data_scale_output)\n",
    "\n",
    "    output=KMeans_fit.transform(data_scale_output)\n",
    "\n",
    "\n",
    "\n",
    "    score=evaluator.evaluate(output)\n",
    "\n",
    "    silhouette_score.append(score)\n",
    "\n",
    "    print(\"Silhouette Score:\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the silhouette scores in a plot\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,10),silhouette_score)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "433b673278398ef95bc72159b2f92bec4e9cd3a1db108ef45b335d464bd0868b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
